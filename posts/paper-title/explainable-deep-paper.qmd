---
title: "Explainable Deep Ensemble Meta-Learning Framework for Brain Tumor Classification Using MRI Images"
description: "An interpretable deep ensemble model for tumor detection in MRI by integrating pre-trained CNNs with a Light Gradient Boosting Machine meta-learner."
subtitle: "*Cancers*"
# categories: [paper]
author: 
  - name: S. Kakon
  - name: S. Chakrabarty
  - name: Z. Al Sazid
  - name: I. A. Begum
  - name: Md Abdus Samad
  - name: A. S. M. S. Hosen
date: "2025-08-30"
categories: [paper]
image: "explainable-deep-dnsemble.jpg"
citation:
  type: article-journal
  container-title: "Cancers"
  doi: "10.3390/cancers17172853"
  available-date: "2025-08-30"
  issued: "2025-08-30"
  volume: 17
  issue: 17
  page: 2853
  url: https://www.mdpi.com/2072-6694/17/17/2853
google-scholar: true 
---

<!--Include academic icons or buttons-->

{{< include /files/includes/_academic.qmd >}}


## Abstract

**Background:** Brain tumors can severely impair neurological function, leading to symptoms such as headaches, memory loss, motor coordination deficits, and visual disturbances. In severe cases, they may cause permanent cognitive damage or become life-threatening without early detection.

**Methods:** To address this, we propose an interpretable deep ensemble model for tumor detection in Magnetic Resonance Imaging (MRI) by integrating pre-trained Convolutional Neural Networks—EfficientNetB7, InceptionV3, and Xception—using a soft voting ensemble to improve classification accuracy. The framework is further enhanced with a Light Gradient Boosting Machine as a meta-learner to increase prediction accuracy and robustness within a stacking architecture. Hyperparameter tuning is conducted using Optuna, and overfitting is mitigated through batch normalization, L2 weight decay, dropout, early stopping, and extensive data augmentation.

**Results:** The proposed model integrates extensive data augmentation and advanced regularization techniques to enhance generalization. Explainable Artificial Intelligence (XAI) methods are incorporated to strengthen clinical trust: Gradient-Weighted Class Activation Mapping++ (Grad-CAM++) provides spatial localization by highlighting MRI regions most influential to predictions. Local Interpretable Model-Agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP) provide complementary interpretability.

**Conclusions:** This transparent and reproducible framework aims to serve as a foundation for the research community, supporting the development of reliable tools for screening, review, and follow-up while reducing diagnostic workload and prioritizing patient safety.


## Keywords

Brain tumor detection, MRI images, deep learning, explainable artificial intelligence, ensemble learning, meta-learning, Grad-CAM++, SHAP, LIME


## Highlights

- Integrates EfficientNetB7, InceptionV3, and Xception using soft voting ensemble
- Light Gradient Boosting Machine (LightGBM) as meta-learner in stacking architecture
- Hyperparameter optimization using Optuna framework
- Explainable AI methods: Grad-CAM++, LIME, and SHAP for clinical interpretability
- Comprehensive regularization: batch normalization, L2 weight decay, dropout, early stopping


## Links

- Published [paper](https://www.mdpi.com/2072-6694/17/17/2853){target="_blank"}
- [PDF](https://www.mdpi.com/2072-6694/17/17/2853/pdf){target="_blank"}


```{=html}
<script type="text/javascript" src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script><div class="altmetric-embed" data-badge-popover="right" data-badge-type="donut" data-doi="10.3390/cancers17172853"></div>

<span class="__dimensions_badge_embed__" data-doi="10.3390/cancers17172853" data-hide-zero-citations="true" data-style="small_circle"></span><script async src="https://badge.dimensions.ai/badge.js" charset="utf-8"></script>
```
